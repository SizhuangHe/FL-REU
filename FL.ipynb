{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SizhuangHe/FL-REU/blob/main/FL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import"
      ],
      "metadata": {
        "id": "EOT1sHDlb02g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "C1I9oQsasKEJ"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import random\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms, utils, datasets\n",
        "from torchsummary import summary\n",
        "\n",
        "# Check assigned GPU\n",
        "# gpu_info = !nvidia-smi\n",
        "# gpu_info = '\\n'.join(gpu_info)\n",
        "# if gpu_info.find('failed') >= 0:\n",
        "#   print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "#   print('and then re-execute this cell.')\n",
        "# else:\n",
        "#   print(gpu_info)\n",
        "\n",
        "# set manual seed for reproducibility\n",
        "seed = 42\n",
        "\n",
        "# general reproducibility\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# gpu training specific\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCliePYOrTv2"
      },
      "source": [
        "# Some Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0FnDQXnrrRz-"
      },
      "outputs": [],
      "source": [
        "input_size = 13\n",
        "num_hidden_neuron = 5\n",
        "num_classes = 1\n",
        "output_size = num_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRsss0MEomi-"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ENC7v7ssoruY"
      },
      "outputs": [],
      "source": [
        "class IRIS_NN(nn.Module):\n",
        "  def __init__(self, input_size, num_hidden_neurons, output_size):\n",
        "    super(IRIS_NN, self).__init__()\n",
        "\n",
        "    self.l1 = nn.Linear(input_size, num_hidden_neurons)\n",
        "    self.l2 = nn.Linear(num_hidden_neurons, num_hidden_neurons)\n",
        "    self.l3 = nn.Linear(num_hidden_neurons, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.flatten(x,1)\n",
        "    x = F.relu(self.l1(x))\n",
        "    x = F.relu(self.l2(x))\n",
        "    out = self.l3(x)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGQ8ZjFiuO2N"
      },
      "source": [
        "# Load DataSet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc2wK2Kqi_i3"
      },
      "source": [
        "## Prepare the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset setup"
      ],
      "metadata": {
        "id": "zmwMaHZjR5A8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RzYGDUJSb3a6"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_csv(\"Iris.csv\")\n",
        "\n",
        "# le=LabelEncoder()\n",
        "# df['Species']=le.fit_transform(df['Species'])\n",
        "\n",
        "# label=df['Species'].values\n",
        "\n",
        "# scaler=StandardScaler()\n",
        "# df = df.drop('Species',axis=1)\n",
        "# df = df.drop('Id', axis=1)\n",
        "# scaled_df=scaler.fit_transform(df)\n",
        "\n",
        "# feature=scaled_df.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6g4zOeMNlA3m"
      },
      "outputs": [],
      "source": [
        "class IrisData(Dataset):\n",
        "  def __init__(self, feature_train, label_train):\n",
        "    self.feature = feature_train\n",
        "    self.label = label_train\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.label) \n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    sample = self.feature[idx]\n",
        "    return sample, self.label[idx]   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform (Add Gaussian Noise)"
      ],
      "metadata": {
        "id": "uHrfce5gRynl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def AddGaussianNoise(dataset, client_dict, client, mean, std):\n",
        "  #loader = DataLoader(CustomDataset(dataset, client_dict[client]), batch_size=len(client_dict[client]), shuffle=True)\n",
        "  \n",
        "  for data, label in CustomDataset(dataset, client_dict[client]):\n",
        "    data += (torch.randn(len(data)) * std + mean).numpy()"
      ],
      "metadata": {
        "id": "G9QqPvECoK1c"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class AddGaussianNoise(object):\n",
        "#     def __init__(self, mean=0., std=1.):\n",
        "#         self.std = std\n",
        "#         self.mean = mean\n",
        "        \n",
        "#     def __call__(self, sample):\n",
        "#       sample += (torch.randn(sample.size) * self.std + self.mean).numpy()\n",
        "#       return sample\n",
        "    \n",
        "#     def __repr__(self):\n",
        "#         return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n"
      ],
      "metadata": {
        "id": "8muC-L2dvJ81"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other built-in dataset"
      ],
      "metadata": {
        "id": "EMMtpEzXWCkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MNIST"
      ],
      "metadata": {
        "id": "t6jcYXTvN8Bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transforms_mnist = transforms.Compose([\n",
        "#                                        transforms.ToTensor(),\n",
        "#                                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "#                                        ])\n",
        "\n",
        "# dataset_train = datasets.MNIST('./data/mnist/', train=True, download=True, transform=transforms_mnist)\n",
        "# dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=transforms_mnist)"
      ],
      "metadata": {
        "id": "zky0KAs0WGwV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### sklearn Boston dataset (Regression)"
      ],
      "metadata": {
        "id": "BiNzuZoxOAPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "feature = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]]).astype(np.float32)\n",
        "label = raw_df.values[1::2, 2].astype(np.float32)\n",
        "\n",
        "scaler=StandardScaler()\n",
        "feature = scaler.fit_transform(feature)"
      ],
      "metadata": {
        "id": "aQhqZRvBOJDM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation"
      ],
      "metadata": {
        "id": "rkpiegKISAms"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2vlcIXlEi-yX"
      },
      "outputs": [],
      "source": [
        "feature_train,feature_test,label_train, label_test=train_test_split(feature, label,test_size=0.20,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5lJqRVJbmM3X"
      },
      "outputs": [],
      "source": [
        "dataset_train = IrisData(feature_train, label_train) # only the __init__ function is called here\n",
        "dataset_test = IrisData(feature_test, label_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hkSAY43yFQ4e"
      },
      "outputs": [],
      "source": [
        "classes_test = [i for i in range(num_classes)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z01-GnrZoGad"
      },
      "source": [
        "# Partition the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TyGhwGCRoBPT"
      },
      "outputs": [],
      "source": [
        "def iid_partition(dataset, clients):\n",
        "  \"\"\"\n",
        "  I.I.D paritioning of data over clients\n",
        "  Shuffle the data\n",
        "  Split it between clients\n",
        "  \n",
        "  params:\n",
        "    - dataset (torch.utils.Dataset): Dataset containing the MNIST Images\n",
        "    - clients (int): Number of Clients to split the data between\n",
        "\n",
        "  returns:\n",
        "    - Dictionary of image indexes for each client\n",
        "  \"\"\"\n",
        "\n",
        "  num_items_per_client = int(len(dataset)/clients)\n",
        "  client_dict = {}\n",
        "  image_idxs = [i for i in range(len(dataset))]\n",
        "\n",
        "  for i in range(clients):\n",
        "    client_dict[i] = set(np.random.choice(image_idxs, num_items_per_client, replace=False))\n",
        "    image_idxs = list(set(image_idxs) - client_dict[i])\n",
        "\n",
        "  return client_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5rDdTgugoK10"
      },
      "outputs": [],
      "source": [
        "def non_iid_partition(dataset, clients, total_shards, shards_size, num_shards_per_client):\n",
        "  \"\"\"\n",
        "  non I.I.D parititioning of data over clients\n",
        "  Sort the data by the digit label\n",
        "  Divide the data into N shards of size S\n",
        "  Randomly assign each client with  shards\n",
        "\n",
        "  params:\n",
        "    - dataset (torch.utils.Dataset): Dataset containing the MNIST Images\n",
        "    - clients (int): Number of Clients to split the data between\n",
        "    - total_shards (int): Number of shards to partition the data in\n",
        "    - shards_size (int): Size of each shard \n",
        "    - num_shards_per_client (int): Number of shards of size shards_size that each client receives\n",
        "\n",
        "  returns:\n",
        "    - Dictionary of image indexes for each client\n",
        "  \"\"\"\n",
        "\n",
        "  \n",
        "  shard_idxs = [i for i in range(total_shards)]\n",
        "  client_dict = {i: np.array([], dtype='int64') for i in range(clients)}\n",
        "  client_dict_out = {}\n",
        "  idxs = np.arange(len(dataset))\n",
        "  #print(f\"idxs: {idxs}\")\n",
        "  \n",
        "  data_labels = dataset.label\n",
        "  #data_labels = dataset.targets.numpy()\n",
        "  #print(f\"data_labels: {data_labels}\")\n",
        "\n",
        "  # sort the labels\n",
        "  label_idxs = np.vstack((idxs, data_labels))\n",
        "  #print(f\"label_idx: {label_idxs}\")\n",
        "  label_idxs = label_idxs[:, label_idxs[1,:].argsort()]\n",
        "  #print(f\"label_idx new: {label_idxs}\")\n",
        "  idxs = label_idxs[0,:].astype(int)\n",
        "  #print(f\"idxs new: {idxs}\")\n",
        "\n",
        "  # divide the data into total_shards of size shards_size\n",
        "  # assign num_shards_per_client to each client\n",
        "  for i in range(clients):\n",
        "    rand_set = set(np.random.choice(shard_idxs, num_shards_per_client, replace=False))\n",
        "    #print(f\"rand_set: {rand_set}\")\n",
        "    shard_idxs = list(set(shard_idxs) - rand_set)\n",
        "    #print(f\"shard_idxs: {shard_idxs}\")\n",
        "\n",
        "    for rand in rand_set:\n",
        "      #print(f\"to append: {idxs[rand*shards_size:(rand+1)*shards_size]}\")\n",
        "      client_dict[i] = np.concatenate((client_dict[i], idxs[rand*shards_size:(rand+1)*shards_size]))\n",
        "\n",
        "      set_i = set(client_dict[i])\n",
        "      client_dict_out[i] = set_i\n",
        "  return client_dict_out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def client_dict_train_val_split(client_dict_in, validation_ratio):\n",
        "  \"\"\"\n",
        "  Split each client data randomly to two parts: training data and validation data\n",
        "\n",
        "  param:\n",
        "    - client_dict_in (python dictionary): the client dictionary\n",
        "    - validation_ratio (double): the proportion of client data to be spitted into validation data\n",
        "\n",
        "  returns:\n",
        "    - dictionary of indices for each client's training and validation data  \n",
        "  \"\"\"\n",
        "  client_dict_out = {}\n",
        "\n",
        "\n",
        "  for client in client_dict_in:\n",
        "    num_total_data = len(client_dict_in[client])\n",
        "    num_val_data = int(num_total_data * validation_ratio)\n",
        "    val_set = set(np.random.choice(np.array(list(client_dict_in[client])), num_val_data, replace=False))\n",
        "    train_set = client_dict_in[client] - val_set\n",
        "    client_dict_out[client]={}\n",
        "    client_dict_out[client][\"train\"] = train_set\n",
        "    client_dict_out[client][\"validation\"] = val_set\n",
        "    \n",
        "  return client_dict_out  "
      ],
      "metadata": {
        "id": "yfyUYnnXPJhy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seeClientMean(client_dict):\n",
        "  \"\"\"\n",
        "  This function returns the mean of each client's local training set\n",
        "  Can use this to roughly see how iid the dictionary is\n",
        "  \"\"\"\n",
        "  mean = []\n",
        "\n",
        "  for client in client_dict:\n",
        "    loader =  DataLoader(CustomDataset(dataset_train, client_dict[client]), batch_size=len(client_dict[client]), shuffle=True)\n",
        "    ld = iter(loader).next()\n",
        "    data, label = ld\n",
        "    for i in range(len(client_dict[client])):\n",
        "      mean.append(torch.mean(label).item())\n",
        "\n",
        "  print(mean)  \n",
        "  print(\"Variance of mean client data: \", np.var(mean))\n",
        "  print(\"Note: a small value is expected for i.i.d partition.\")"
      ],
      "metadata": {
        "id": "Z63ilDSugASQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHVh_TAWufaK"
      },
      "source": [
        "# Client Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Qy9_1kKcuiDb"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, dataset, idxs):\n",
        "      self.dataset = dataset\n",
        "      self.idxs = list(idxs)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.idxs)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "      image, label = self.dataset[self.idxs[item]]\n",
        "      return image, label\n",
        "\n",
        "class ClientUpdate(object):\n",
        "  def __init__(self, dataset, batchSize, learning_rate, epochs, idxs):\n",
        "    self.train_loader = DataLoader(CustomDataset(dataset, idxs), batch_size=batchSize, shuffle=True)\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.epochs = epochs\n",
        "\n",
        "  def train(self, model):\n",
        "\n",
        "    #criterion = nn.CrossEntropyLoss()\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.5)\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    e_loss = []\n",
        "\n",
        "    \n",
        "\n",
        "    for epoch in range(1, self.epochs+1):\n",
        "\n",
        "      train_loss = 0.0\n",
        "\n",
        "      model.train()\n",
        "      for data, labels in self.train_loader:\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "        # clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # make a forward pass\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, labels)\n",
        "        # do a backwards pass\n",
        "        loss.backward()\n",
        "        # perform a single optimization step\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "\n",
        "      # average losses\n",
        "      train_loss = train_loss/len(self.train_loader.dataset)\n",
        "      e_loss.append(train_loss)\n",
        "\n",
        "    total_loss = sum(e_loss)/len(e_loss)\n",
        "\n",
        "    return model.state_dict(), total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IvnLnucumpj"
      },
      "source": [
        "# Server Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "H-IAyGOAusQk"
      },
      "outputs": [],
      "source": [
        "def training(model, rounds, batch_size, num_classes,lr, ds, data_dict, C, K, E, plt_title, plt_color, report_accuracy):\n",
        "  \"\"\"\n",
        "  Function implements the Federated Averaging Algorithm from the FedAvg paper.\n",
        "  Specifically, this function is used for the server side training and weight update\n",
        "\n",
        "  Params:\n",
        "    - model:           PyTorch model to train\n",
        "    - rounds:          Number of communication rounds for the client update\n",
        "    - batch_size:      Batch size for client update training\n",
        "    - lr:              Learning rate used for client update training\n",
        "    - ds:              Dataset used for training\n",
        "    - data_dict:       Type of data partition used for training (IID or non-IID)\n",
        "    - C:               Fraction of clients randomly chosen to perform computation on each round\n",
        "    - K:               Total number of clients\n",
        "    - E:               Number of training passes each client makes over its local dataset per round\n",
        "    - tb_writer_name:  Directory name to save the tensorboard logs\n",
        "  Returns:\n",
        "    - model:           Trained model on the server\n",
        "  \"\"\"\n",
        "\n",
        "  # global model weights\n",
        "  global_weights = model.state_dict()\n",
        "\n",
        "  # training loss\n",
        "  train_loss = []\n",
        "  \n",
        "  \n",
        "  # measure time\n",
        "  start = time.time()\n",
        "\n",
        "  for curr_round in range(1, rounds+1):\n",
        "    w, local_loss = [], []\n",
        "\n",
        "    m = max(int(C*K), 1)\n",
        "    \n",
        "    S_t = np.random.choice(range(K), m, replace=False) # S_t is the array of clients to train on\n",
        "    \n",
        "    for k in S_t:\n",
        "      local_update = ClientUpdate(dataset=ds, batchSize=batch_size, learning_rate=lr, epochs=E, idxs=data_dict[k][\"train\"])\n",
        "      weights, loss = local_update.train(model=copy.deepcopy(model))\n",
        "\n",
        "      w.append(copy.deepcopy(weights))\n",
        "      local_loss.append(copy.deepcopy(loss))\n",
        "\n",
        "    # updating the global weights\n",
        "    weights_avg = copy.deepcopy(w[0])\n",
        "    for k in weights_avg.keys():\n",
        "      for i in range(1, len(w)):\n",
        "        weights_avg[k] += w[i][k]\n",
        "\n",
        "      weights_avg[k] = torch.div(weights_avg[k], len(w)) # take into account each client's number of data\n",
        "\n",
        "    global_weights = weights_avg\n",
        "\n",
        "    # move the updated weights to our model state dict\n",
        "    model.load_state_dict(global_weights)\n",
        "\n",
        "    # loss\n",
        "    loss_avg = sum(local_loss) / len(local_loss)\n",
        "    print('Round: {}... \\tAverage Loss: {}'.format(curr_round, round(loss_avg, 3)))\n",
        "    train_loss.append(loss_avg)\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    criterion = nn.MSELoss()\n",
        "    print(\"-> Validation\\n\")\n",
        "    for k in S_t:\n",
        "      print(f\"Validation on Client {k}\")\n",
        "      curr_round_val_loss = validate(model, dataset_train, data_dict[k][\"validation\"], criterion, num_classes, classes_test, report_accuracy)\n",
        "    \n",
        "\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  fig_train, ax_train = plt.subplots()\n",
        "  x_axis_train = np.arange(1, rounds+1)\n",
        "  y_axis_train = np.array(train_loss)\n",
        "  ax_train.plot(x_axis_train, y_axis_train, 'tab:'+plt_color)\n",
        "\n",
        "  ax_train.set(xlabel='Number of Rounds', ylabel='Train Loss',\n",
        "       title=plt_title)\n",
        "  ax_train.grid()\n",
        "  fig_train.savefig(plt_title+'.jpg', format='jpg')\n",
        "\n",
        "  # fig_val, ax_val = plt.subplots()\n",
        "  # x_axis_val = np.arange(1, rounds+1)\n",
        "  # y_axis_val = np.array(val_loss)\n",
        "  # ax_val.plot(x_axis_val, y_axis_val, 'tab:'+plt_color)\n",
        "\n",
        "  # ax_val.set(xlabel='Number of Rounds', ylabel='Train Loss',\n",
        "  #      title=plt_title)\n",
        "  # ax_val.grid()\n",
        "  # fig_val.savefig(plt_title+'.jpg', format='jpg')\n",
        "\n",
        "  print(\"Training Done!\")\n",
        "  print(\"Total time taken to Train: {}\".format(end-start))\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, dataset, val_idx, criterion,num_classes, classes, report_accuracy):\n",
        "  batch_size = len(val_idx)\n",
        "  val_loader = DataLoader(CustomDataset(dataset, val_idx), batch_size=len(val_idx), shuffle=True)\n",
        "  return test(model, dataset, batch_size, criterion, num_classes, classes, val_loader, \"Validation\", report_accuracy=report_accuracy)"
      ],
      "metadata": {
        "id": "ApLY9XfGa53E"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiaZl4R0u3eW"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, dataset, bs, criterion, num_classes, classes, loader, mode, report_accuracy):\n",
        "  test_loss = 0.0\n",
        "  correct_class = list(0. for i in range(num_classes))\n",
        "  total_class = list(0. for i in range(num_classes))\n",
        "\n",
        "  for data, labels in loader:\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    test_loss += loss.item()*data.size(0)\n",
        "\n",
        "    if report_accuracy == True:\n",
        "      _, pred = torch.max(output, 1)\n",
        "      correct_tensor = pred.eq(labels.data.view_as(pred))\n",
        "      correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
        "      #print(f\"data len: {len(data)}\")\n",
        "      #test accuracy for each object class\n",
        "      for i in range(len(data)):\n",
        "        label = labels.data[i]\n",
        "        correct_class[label] += correct[i].item()\n",
        "        total_class[label] += 1\n",
        "        #print(f\"i: {i}, label: {label}\")\n",
        "\n",
        "   \n",
        "    \n",
        "  # avg test loss\n",
        "  test_loss = test_loss/len(loader.dataset)\n",
        "  \n",
        "  print(mode, \"Loss: {:.6f}\\n\".format(test_loss))\n",
        "\n",
        "  if report_accuracy == True:\n",
        "    # print test accuracy\n",
        "    for i in range(num_classes):\n",
        "      if total_class[i]>0:\n",
        "        print(mode, 'Accuracy of %5s: %2d%% (%2d/%2d)' % \n",
        "            (classes[i], 100 * correct_class[i] / total_class[i],\n",
        "            np.sum(correct_class[i]), np.sum(total_class[i]))) \n",
        "\n",
        "      else:\n",
        "        print(mode, 'Accuracy of %5s: N/A (no '% (classes[i]), mode, ' examples)' )\n",
        "      \n",
        "    print('\\nFinal ', mode, ' Accuracy: {:.3f} ({}/{})\\n'.format(\n",
        "        100. * np.sum(correct_class) / np.sum(total_class),\n",
        "        np.sum(correct_class), np.sum(total_class)))\n",
        "\n",
        "  return test_loss"
      ],
      "metadata": {
        "id": "hz_rUy-VlDG8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tB2TnXZ5u43W"
      },
      "outputs": [],
      "source": [
        "def testing(model, dataset, bs, criterion, num_classes, classes, report_accuracy):\n",
        "  test_loader = DataLoader(dataset, batch_size=bs)\n",
        "  \n",
        "  model.eval()\n",
        "  test(model, dataset, bs, criterion, num_classes, classes, test_loader, \"Test\", report_accuracy=report_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8IhkPvxu5vG"
      },
      "source": [
        "# IID Case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMQ-N9gsEJvF"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wZzpnCbNvH8w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c4f5c38-50c9-4dc0-ab01-33785088a023"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variance of mean client data:  1.3139819551708933\n",
            "Note: a small value is expected for i.i.d partition.\n"
          ]
        }
      ],
      "source": [
        "# number of training rounds\n",
        "rounds = 100\n",
        "# client fraction\n",
        "C = 1\n",
        "# number of clients\n",
        "K = 10\n",
        "# number of training passes on local dataset for each round\n",
        "E = 10\n",
        "# batch size\n",
        "batch_size = 10\n",
        "# learning Rate\n",
        "lr=0.01\n",
        "# data partition dictionary\n",
        "client_dict = iid_partition(dataset_train, K)\n",
        "seeClientMean(client_dict)\n",
        "iid_dict = client_dict_train_val_split(client_dict_in=client_dict, validation_ratio=0.2)\n",
        "# load model\n",
        "model_iid = IRIS_NN(input_size, num_hidden_neuron, output_size)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model_iid.cuda()\n",
        "\n",
        "#model_iid_trained = training(model_iid, rounds, batch_size, num_classes, lr, dataset_train, iid_dict, C, K, E, \"on IID Dataset\", \"orange\", False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crvgQe0HELwo"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Chbf7e27EPgF",
        "outputId": "03cde3d7-a565-43b6-fe12-e36dea4f45ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-ded25678374a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtesting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_iid_trained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model_iid_trained' is not defined"
          ]
        }
      ],
      "source": [
        "criterion = nn.MSELoss()\n",
        "testing(model_iid_trained, dataset_test, K, criterion, num_classes, classes_test, False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIQHWWymDWif"
      },
      "source": [
        "# Non IID training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6FlzgLR8ECaG",
        "outputId": "8016d115-8b06-46aa-8e88-f3d8c99ed890",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variance of mean client data:  75.92970122368352\n",
            "Note: a small value is expected for i.i.d partition.\n"
          ]
        }
      ],
      "source": [
        "# number of training rounds\n",
        "rounds = 100\n",
        "# client fraction\n",
        "C = 1\n",
        "# number of clients\n",
        "K = 10\n",
        "# number of training passes on local dataset for each round\n",
        "E = 10\n",
        "# batch size\n",
        "batch_size = 10\n",
        "# learning Rate\n",
        "lr=0.01\n",
        "# data partition dictionary\n",
        "client_dict = non_iid_partition(dataset_train, clients=K, total_shards=10, shards_size=40, num_shards_per_client=1) # every client is given only one shard, which is very non_iid \n",
        "seeClientMean(client_dict)\n",
        "non_iid_dict = client_dict_train_val_split(client_dict_in=client_dict, validation_ratio=0.2)\n",
        "\n",
        "# loader =  DataLoader(CustomDataset(dataset_train, client_dict[0]), batch_size=len( client_dict[0]), shuffle=True)\n",
        "# ld = iter(loader).next()\n",
        "# data, label = ld\n",
        "# print(\"data: \", data, \"label: \", label)\n",
        "#load model\n",
        "model_non_iid = IRIS_NN(input_size, num_hidden_neuron, output_size)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model_non_iid.cuda()\n",
        "\n",
        "#model_non_iid_trained = training(model_non_iid, rounds, batch_size, num_classes,lr, dataset_train, non_iid_dict, C, K, E, \"on non IID Dataset\", \"orange\", False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLKjiwnlwmoA"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LntuHfeOw-ws"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "testing(model_non_iid_trained, dataset_test, 10, criterion, num_classes, classes_test, False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "FL.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "12d30b77a464acd39dc57a4d74939e0be500310b5b75c20a10dcf54a6539ac33"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('MYfedtorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}